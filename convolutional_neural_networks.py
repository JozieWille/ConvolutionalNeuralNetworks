# -*- coding: utf-8 -*-
"""Convolutional Neural Networks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1St-xJDENSn8665O9dWGQ3aX6wRNYUp07

# Homework 1, MSBC.5190 Modern Artificial Intelligence S23

**Teammates: Jozie Wille, Lexi Dingle, Charlie Rudy**

**Teamname: 001.7**

Handout 02/10/2023 4pm, **due 02/24/2022 by 4pm**. Please submit through Canvas. Each team only needs to submit one copy.

Important information about submission:
- Write all code, text (answers), and figures in the notebook.
- Please make sure that the submitted notebook has been run and the cell outputs are visible.
- Please print the notebook as PDF and submit it together with the notebook. Your submission should contain two files: `homework1-teamname.ipynb` and `homework1-teamname.pdf`

In this homework, we will build and train convolutional neural networks (CNN) on a subset of [Kaggle "cats vs. dog"](https://www.kaggle.com/c/dogs-vs-cats) classification data. The goal of the homework are four folds:


1.   Train a small CNN from scratch as a baseline 
2.   Improve the baseline model. You can find some hints in the last section.
3.   Implement transfer learning: fine-tune top layers of a pre-trained network
4.   Experiment and develop a better model (i.e., better accuracy). You can find some hints in the last section.

First, import the packages or modules required for this homework.
"""

################################################################################
# TODO: Fill in your codes                                                     #
# Import packages or modules                                                   #
################################################################################
import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Conv2D, MaxPooling2D # new!
from keras.layers import Flatten
from keras.layers import Dense, Dropout, BatchNormalization

from google.colab import drive
drive.mount('/content/drive')

"""## Obtain and Organize the Data Sets

The competition data is divided into training set (25,000 images) and testing set (10,000 images). For this homework, we are going to use 4,000 images (2,000 cat images and 2,000 dog images) from the original training data for training, 1,000 images for validation, and 1,000 images for testing.


### Download the Data Set###
The small data set `dogs-vs-cats-small.zip` can be downloaded through Canvas. After downloading the data, upload it to Google Colab. The easiest way to do it is to use the upload option at the top of the file-explorer pane. Note that files uploaded in this way are uploaded to the running instance and will be deleted when the session is disconnected. Alternatively, you can also upload the data set to your Google Drive and mount your Google Drive to Colab. (More info: https://colab.research.google.com/notebooks/io.ipynb)

### Unzip the Data Set###
"""

!unzip '/content/drive/MyDrive/MSBC5190:ModernAI/dogs-vs-cats-small.zip'

"""Here, I assume that you upload `dogs-vs-cats-small.zip` to the running instance of Google Colab. I have already organized the data to training, validation, and testing dataset. Under each of the data directories, each subdirectory represent a class and contains images from that class. After unzipping, you will find the training, validation, and testing data set in the following paths:

* dogs-vs-cats-small/
  * train/
    * cat/
    * dog/
  * validation/
    * cat/
    * dog/
  *test/
    * cat/
    * dog/

### Read and Preprocess the Data

We use [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) to load images on the fly from disk. You can use and modify it later to perform random data augmentations to improve model performance. Here, we rescale the data to between 0 and 1 (by multiplying by 1/255)

Note: Here we are following our textbook and use ImageDataGenerator to load images and perform data augmentation. Alternatively, you can also use tf.keras.utils.image_dataset_from_directory. 

*   https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory
*   https://www.tensorflow.org/tutorials/load_data/images
"""

data_dir = './dogs-vs-cats-small/'

from keras.preprocessing.image import ImageDataGenerator
# Instantiate three image generator classes:
train_datagen = ImageDataGenerator(
    rescale=1.0/255,
    data_format='channels_last')

valid_datagen = ImageDataGenerator(
    rescale=1.0/255,
    data_format='channels_last')

test_datagen = ImageDataGenerator(
    rescale=1.0/255,
    data_format='channels_last'
)

batch_size=32
train_generator = train_datagen.flow_from_directory(
    directory=data_dir + 'train',
    target_size=(224, 224),
    classes=['cat','dog'],
    class_mode='categorical',
    batch_size=batch_size,
    shuffle=True,
    seed=42)

valid_generator = valid_datagen.flow_from_directory(
    directory=data_dir + 'validation',
    target_size=(224, 224),
    classes=['cat','dog'],
    class_mode='categorical',
    batch_size=batch_size,
    shuffle=True,
    seed=42)

test_generator = test_datagen.flow_from_directory(
    directory=data_dir + 'test',
    target_size=(224, 224),
    classes=['cat','dog'],
    class_mode='categorical',
    batch_size=batch_size,
    shuffle=True,
    seed=42)

"""## 1. Build and Train a Baseline Model (40%)

We will start with building a simple convolutional neural network. 

### Define the model

Please define a model with the following layers and hyperparameters.
* Input image: 224 (height) x 224 (width) x 3 (channels)
* Convolution: 16 kernels (size = 3 x 3), stride = 1, padding = 1
* MaxPool: kernel size = 2 x 2, stride = 2, padding = 0
* Convolution: 32 kernels (size = 3 x 3), stride = 1, padding = 1
* MaxPool: kernel size = 2 x 2, stride = 2, padding = 0
* Convolution: 64 kernels (size = 3 x 3), stride = 1, padding = 1
* MaxPool: kernel size = 2 x 2, stride = 2, padding = 0
* Dense: 128 fully connected neurons
* Output: 1 of 2 classes (cat or dog)

"""

################################################################################
# TODO: Fill in your codes                                                     #
# Define the model           
# same means there is padding, calculate shape of output 
#do not necessarily need padding = valid because default for MaxPooling 
# use dropout and batchnormalization in next section        #
################################################################################
model = Sequential()

model.add(Conv2D(16, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', 
                 input_shape=(224, 224, 3)))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'valid'))

model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', strides=(1,1), padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), strides = (1,1), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

model.add(Flatten())
model.add(Dense(128, activation='relu'))

model.add(Dense(2, activation='softmax'))

model.summary()

"""**Inline Question #1:** Notice the output shape and # of param of each layer

- What is the output shape of the first convolutional layer and how to calculate it?
- What is the # of params of the first convolutional layer and how to calculate it?

**Your Answer:** *The output shape of the first convolutional layer is (112, 112, 16).
We calculate that by taking the kernel size of max-pooling layer which is (2,2) and stride is 2, so output size is (224â€“2)/2 +1 = 112.
The first convolutional layer has 448 parameters. We calculate that by multiplying the width of the filter and the height of the filter times the number of filters plus 1.*

**Formula:** Output_Shape = (image_dimension - filter_dimension + 2*padding) / stride+1

### Configure and Train the Baseline Model

Please use Adam as the optimizer and keep track of accuracy metric.

**Inline Question #2:** What loss function will you choose for this classfication problem?

**Your Answer:** *We will use Binary Cross Entropy because this is a binary classification problem.*
"""

################################################################################
# TODO: Fill in your codes                                                     #
# Configure and train the model (set number of epochs to 10)                   #
################################################################################

# check if mse is correct loss function for binary classification
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# ask if we should be using history or history_t below
# need to know what variables to use instead of X and Y
## history = model.fit(X, Y, batch_size=64, epochs=10, verbose=1, 
                    # validation_split=0.2, shuffle=True)

history_t = model.fit(train_generator, steps_per_epoch=15, 
                    epochs=10, validation_data=valid_generator, 
                    validation_steps=15)

"""**Inline Question #3:** Please draw loss and accuracy learning curves on the training and validation set.
- Please explain your observation of the learning curves.

**Your Answer:** *The learning curves bounce up and down quite a lot leading to unreliable results but the overall trends that you can see is that as the epochs increase, the accuracy also increase and the model loss decreases.*
"""

################################################################################
# TODO: Fill in your codes                                                     #
# Draw learning curves                                                         #
################################################################################
def plot_learning_curve(history):
  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  plt.title('model accuracy')
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.show()

  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('model loss')
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.show()

plot_learning_curve(history_t)

"""### Evaluate Model Performance

**Inline Question #4:** What is the accuracy on the test dataset?

**Your Answer:** *The accuracy on the test dataset is 0.6450.*
"""

################################################################################
# TODO: Fill in your codes                                                     #
# Calculate accuracy on the test data set                                      #
################################################################################
# Use the evaluate() function
test_generator.reset()
test_loss, test_accuracy = model.evaluate(test_generator,
                                          steps=test_generator.n/test_generator.batch_size,
                                          verbose=1)

"""## Improve the Baseline Model (10%)

**Inline Question #5:** Please propose and implement one improvement on the baseline model. 
* What is the rational for the proposed improvement?
* Did it help? Please present your evidence.

**Answer**: Batch Normalization will make our model more stable through normalization of the layer's inputs by recentering and rescaling. This did not help the accuracy, the new model accuracy is 0.5.
"""

################################################################################
# TODO: Fill in your codes                                                     #
# You need to finish three things                                              #
# 1. Implement the model improvement                                           #
# 2. Draw learning curves                                                      #
# 3. Evaluate model performance on test data                                   #
# Note: You do not have to put all the codes in this cell and can write in     #
# multiple cells.                                                              #
################################################################################

model2 = Sequential()

model2.add(Conv2D(16, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', 
                 input_shape=(224, 224, 3)))
model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'valid'))
model2.add(BatchNormalization())

model2.add(Conv2D(32, kernel_size=(3, 3), activation='relu', strides=(1,1), padding='same'))
model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model2.add(BatchNormalization())

model2.add(Conv2D(64, kernel_size=(3, 3), strides = (1,1), activation='relu', padding='same'))
model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model2.add(BatchNormalization())

model2.add(Flatten())
model2.add(Dense(128, activation='relu'))

model2.add(Dense(2, activation='softmax'))

model2.summary()

# check if mse is correct loss function for binary classification
model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# ask if we should be using history or history_t below
# need to know what variables to use instead of X and Y
## history = model.fit(X, Y, batch_size=64, epochs=10, verbose=1, 
                    # validation_split=0.2, shuffle=True)

history_t2 = model2.fit(train_generator, steps_per_epoch=15, 
                    epochs=10, validation_data=valid_generator, 
                    validation_steps=15)

def plot_learning_curve(history):
  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  plt.title('model accuracy')
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.show()

  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('model loss')
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.show()

plot_learning_curve(history_t2)

# Use the evaluate() function
test_generator.reset()
test_loss2, test_accuracy2 = model2.evaluate(test_generator,
                                          steps=test_generator.n/test_generator.batch_size,
                                          verbose=1)

"""## 3. Transfer Learning (30%)

Next, we are going to fine-tune [VGG19](https://arxiv.org/abs/1409.1556) on our small dogs-vs-cats dataset. Specifically, we will load the VGG19 model pre-trained on ImageNet, freeze its weights, add some new layers on top of the frozen layers from VGG19, and train the new layers on our dataset. You need to add an output classification layer on top of the base VGG19 model. Please also add a `Dropout` layer with dropout rate = 0.5 before the classification layer for regularization.

**Inline Question #6:** Please implement the transfer learning, draw learning curves, and report model performance on test data.  
* Please explain your observation of the learning curves.
* What is the model performance on the test data? Is the better than the baseline model? Why is the performance better or worse than before?

**Your Answer:** *The learning curves show that as the number of epochs increase the accuracy also increases and the model loss decreases. The model performance on the test data shows an accuracy of 0.9187. This is much better than the baseline model because we included the VGG19 model for Keras.*
"""

################################################################################
# TODO: Fill in your codes                                                     #
# You need to finish three things                                              #
# 1. Implement transfer learning: define, configure, and train model           #
# 2. Draw learning curves                                                      #
# 3. Evaluate model performance on test data                                   #
# Note: You do not have to put all the codes in this cell and can write in     #
# multiple cells.                                                              #
################################################################################

from keras.applications.vgg19 import VGG19
from keras.preprocessing.image import ImageDataGenerator

vgg19 = VGG19(include_top=False,
              weights='imagenet',
              input_shape=(224,224,3),
              pooling=None)

for layer in vgg19.layers:
    layer.trainable = False

# Instantiate the sequential model and add the VGG19 model: 
model_3 = Sequential()
model_3.add(vgg19)

# Add the custom layers atop the VGG19 model: 
model_3.add(Flatten(name='flattened'))
model_3.add(Dropout(0.5, name='dropout'))
model_3.add(Dense(2, activation='softmax', name='predictions'))

model_3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history_3 = model_3.fit(train_generator, steps_per_epoch=15, 
                    epochs=10, validation_data=valid_generator, 
                    validation_steps=15)

plot_learning_curve(history_3)

"""## 4. Winning Model and Model Performance (20%)

You probably have noticed that transfer learning achieved a better prediction performance on the test data. In this section, please experiment with more ideas to further improve model performance. 

When experimenting with different model improvement ideas, please only use training and validation data. After you selecting a winning model based on its performance on the validation data, evaluate the winning model's performance on the test data and report it here.

Note that this section is worth 20% of your total grade. Half of it (i.e., 10%) is based on implementation (i.e., implement one improvement) and the other half is based on performance. Teams with higher performance scores get higher grade. 

If you experiment with more than one idea, you do not need to submit all experiment codes and results but just the winning one. You can definitely talk about them in the answers below. 


**Inline Question #7:** How would you improve the model performance further?  
* What did you try and what did you find?
* What is the rational behind the winning model?
* What is the winning model's performance on the test data?

**Answer:** In order to improve the model performance further, we experimented with three different models. The first model we tried was the InceptionV3 model with adam optimizer and a 0.5 dropout layer. It resulted in an accuracy of about 0.7. We also tried a ResNet50 model that reulted in about the same accuarcy as InceptionV3. The last one was VGG19 with added batch normalization. This resulted in our best accuracy of the models we tried for part 4 with an accuracy of 0.93. Because we got a high accuracy from the VGG19 model originally, we wanted to try and optimize that by trying batch normalization and number of epochs. Additional models that we tried included ResNet101V2, DenseNet169, VGG16, VGG19,  and DenseNet201. With these models, we found that VGG19 has 3 more convolutional layers than VGG16 which is why we then used VGG19, which is considered one of the best methods because it produces faster training speed, fewer training samples per time, and higher accuracy. Additionally, we found that even though, ResNet is much deeper than VGG16 and VGG19, the model size is actually substantially smaller due to the usage of global average pooling rather than fully-connected layers. *
"""

################################################################################
# TODO: Fill in your codes                                                     #
# You need to include three things                                             #
# 1. Implement one improvement of the model                                    #
# 2. Draw learning curves                                                      #
# 3. Evaluate model performance on test data                                   #
# Note: You do not have to put all the codes in this cell and can write in     #
# multiple cells.                                                              #
################################################################################

from keras.applications.vgg19 import VGG19
from keras.preprocessing.image import ImageDataGenerator

vgg19 = VGG19(include_top=False,
              weights='imagenet',
              input_shape=(224,224,3),
              pooling=None)

for layer in vgg19.layers:
    layer.trainable = False

# Instantiate the sequential model and add the VGG19 model: 
model_8 = Sequential()
model_8.add(vgg19)

# Add the custom layers atop the VGG19 model: 
model_8.add(BatchNormalization())
model_8.add(Flatten(name='flattened'))
model_8.add(Dropout(0.5, name='dropout'))
model_8.add(Dense(2, activation='softmax', name='predictions'))

model_8.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history_4 = model_8.fit(train_generator, steps_per_epoch=15, 
                    epochs=10, validation_data=valid_generator, 
                    validation_steps=15)

plot_learning_curve(history_4)

test_generator.reset()
test_loss8, test_accuracy8 = model_8.evaluate(test_generator,
                                          steps=test_generator.n/test_generator.batch_size,
                                          verbose=1)

"""## Hints to Improve Model Performance


*   Try different batch_size and num_epochs
*   Try batch normalization, dropout, regularization (check textbook DIL Chapter 9 "Unstable Gradients", "Modern Generalization")
*   Try different optimzers, learning_rate (e.g., learning rate decay, check textbook DIL Chapter 9 "Fancy Optimizers")
*   Try data augmentation using ImageDataGenerator (check textbook DIL Chapter 9 "Data Augmentation" and Chapter 10 Example 10.8)
*   Try different pre-trained models (check https://keras.io/api/applications/)
*   Try a round of fine-tuning of the entire model instead of just the final classification layer (check https://keras.io/guides/transfer_learning/#the-typical-transferlearning-workflow)
*   You probably need to use `tf.keras.callbacks.ModelCheckpoint` and `tf.keras.callbacks.EarlyStopping` to help decide when to stop training and store the best model. https://keras.io/api/callbacks/


"""